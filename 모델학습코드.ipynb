{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binibinibini/project2/blob/main/%EB%AA%A8%EB%8D%B8%ED%95%99%EC%8A%B5%EC%BD%94%EB%93%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWL7DYIJ_bYU"
      },
      "outputs": [],
      "source": [
        "# 1. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q transformers[torch] datasets pandas scikit-learn accelerate\n",
        "\n",
        "# 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEeCwJWEnJVG"
      },
      "outputs": [],
      "source": [
        "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"âœ… Google Driveê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Google Drive ë§ˆìš´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3L8WWQVCtp8"
      },
      "outputs": [],
      "source": [
        "def get_positive_integer(prompt_message):\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìì—ê²Œ 0ì„ ì´ˆê³¼í•˜ëŠ” ìì—°ìˆ˜ë¥¼ ì…ë ¥ë°›ëŠ” í•¨ìˆ˜.\n",
        "    ì˜ëª»ëœ ì…ë ¥(ìŒìˆ˜, 0, ì†Œìˆ˜, ë¬¸ì)ì´ ë“¤ì–´ì˜¤ë©´ ë‹¤ì‹œ ì…ë ¥ë°›ë„ë¡ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(prompt_message)\n",
        "            number = int(user_input)\n",
        "\n",
        "            # ë³€ê²½ëœ ë¶€ë¶„: numberê°€ 0ë³´ë‹¤ í°ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "            if number > 0:\n",
        "                return number\n",
        "            else:\n",
        "                print(\"âš ï¸ ì˜¤ë¥˜: 0ë³´ë‹¤ í° ìì—°ìˆ˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "        except ValueError:\n",
        "            print(\"âš ï¸ ì˜¤ë¥˜: ìœ íš¨í•œ ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "\n",
        "epochs = get_positive_integer(\"epochë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
        "\n",
        "# 2. íŒŒì¼ ê²½ë¡œ ì„¤ì • (â˜…â˜…ì‚¬ìš©ì ìˆ˜ì • í•„ìš”â˜…â˜…)\n",
        "# í•™ìŠµì— ì‚¬ìš©í•  CSV íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ì •í™•í•˜ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n",
        "csv_file_path = '/content/drive/MyDrive/ITWILL(á„Œá…®á†¼á„€á…¡á†«á„‘á…³á†¯á„Œá…¦á†¨2)/á„‘á…¡á„‹á…µá†«á„á…²á†«á„ƒá…³ á„‡á…®á†«á„…á…² á„†á…©á„ƒá…¦á†¯/á„†á…©á„ƒá…¦á†¯ á„‘á…¡á„‹á…µá†«á„á…²á„‚á…µá†¼/á„á…©á†¼á„’á…¡á†¸_á„€á…¡á†·á„Œá…¥á†¼á„‡á…®á†«á„…á…²á„‹á…ªá†«á„…á…­_ìµœì¢….csv'\n",
        "\n",
        "# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ì €ì¥ë  í´ë” ê²½ë¡œ\n",
        "output_dir = f'/content/drive/MyDrive/ITWILL(á„Œá…®á†¼á„€á…¡á†«á„‘á…³á†¯á„Œá…¦á†¨2)/á„‘á…¡á„‹á…µá†«á„á…²á†«á„ƒá…³ á„‡á…®á†«á„…á…² á„†á…©á„ƒá…¦á†¯/kote-finetuned-model-{epochs}epoch'\n",
        "\n",
        "print(f\"ğŸ“‚ í•™ìŠµìš© CSV íŒŒì¼ ê²½ë¡œ: {csv_file_path}\")\n",
        "print(f\"ğŸ’¾ ëª¨ë¸ ì €ì¥ í´ë”: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZXYYTHvAuuV"
      },
      "outputs": [],
      "source": [
        "# 1. CSV íŒŒì¼ ë¡œë“œ\n",
        "try:\n",
        "    # 'utf-8-sig' ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ì„ ì½ë„ë¡ ìˆ˜ì •\n",
        "    df = pd.read_csv(csv_file_path, encoding='utf-8-sig')\n",
        "    print(\"âœ… CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ!\")\n",
        "    print(f\"Original data size: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file_path}\")\n",
        "    print(\"2ë‹¨ê³„ì˜ 'csv_file_path' ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    raise\n",
        "except UnicodeDecodeError:\n",
        "    print(f\"âŒ ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ 'CSV UTF-8' í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ ì €ì¥í•œ í›„ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
        "    raise\n",
        "\n",
        "# 2. í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ ë° ì „ì²˜ë¦¬\n",
        "emotion_labels = ['ê¸°ì¨', 'ê¸°ëŒ€', 'ì• ì •', 'ìŠ¬í””', 'ë¶„ë…¸', 'ê³µí¬']\n",
        "# 'ì¢‹ì•„ìš”ìˆ˜'ì™€ 'ë‹µê¸€ìˆ˜'ë¥¼ í¬í•¨í•˜ì—¬ ì»¬ëŸ¼ ì„ íƒ\n",
        "required_cols = ['ëŒ“ê¸€ë‚´ìš©', 'ì¢‹ì•„ìš”ìˆ˜', 'ë‹µê¸€ìˆ˜'] + emotion_labels\n",
        "df = df[required_cols].copy()\n",
        "\n",
        "# 3. 'ì¢‹ì•„ìš”ìˆ˜', 'ë‹µê¸€ìˆ˜'ì˜ ë¹ˆ ê°’ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°\n",
        "df['ì¢‹ì•„ìš”ìˆ˜'] = df['ì¢‹ì•„ìš”ìˆ˜'].fillna(0).astype(int)\n",
        "df['ë‹µê¸€ìˆ˜'] = df['ë‹µê¸€ìˆ˜'].fillna(0).astype(int)\n",
        "\n",
        "# 4. [ìˆ˜ì •] ê°ì • ë ˆì´ë¸” ì»¬ëŸ¼ì˜ íƒ€ì…ì„ `float`ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
        "df[emotion_labels] = df[emotion_labels].fillna(0).astype(float)\n",
        "\n",
        "# 5. ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒˆë¡œìš´ 'text' ì»¬ëŸ¼ ìƒì„±\n",
        "df['text'] = df.apply(\n",
        "    lambda row: f\"[ì¢‹ì•„ìš”: {row['ì¢‹ì•„ìš”ìˆ˜']}, ë‹µê¸€: {row['ë‹µê¸€ìˆ˜']}] {row['ëŒ“ê¸€ë‚´ìš©']}\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# 6. 'labels' ì»¬ëŸ¼ì„ ë‹¤ì¤‘ ë ˆì´ë¸” í˜•ì‹ìœ¼ë¡œ ìƒì„±\n",
        "df['labels'] = df[emotion_labels].values.tolist()\n",
        "df = df[['text', 'labels']]\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ëŠ” í–‰ ì œê±°\n",
        "df.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "print(\"\\nâœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ. ë ˆì´ë¸” íƒ€ì…ì„ floatìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\")\n",
        "print(\"--- Example of the new text format ---\")\n",
        "print(df.head())\n",
        "\n",
        "# 7. Pandas DataFrameì„ Hugging Face Datasetìœ¼ë¡œ ë³€í™˜\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# 8. í›ˆë ¨(train) ë° ê²€ì¦(validation) ë°ì´í„°ì…‹ìœ¼ë¡œ ë¶„ë¦¬ (80:20)\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "print(\"\\nâœ… Train/validation split complete:\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p268SwLsDJls"
      },
      "outputs": [],
      "source": [
        "# 1. ë² ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ ì •ì˜\n",
        "model_name = \"searle-j/kote_for_easygoing_people\"\n",
        "\n",
        "# 2. í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3. ë ˆì´ë¸” ì •ë³´ ì •ì˜\n",
        "labels = emotion_labels\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "# 4. ëª¨ë¸ ë¡œë“œ (ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ìš©ìœ¼ë¡œ ì„¤ì •)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    problem_type=\"multi_label_classification\",\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True  # ì´ ë¶€ë¶„ì„ ì¶”ê°€í•˜ì—¬ í¬ê¸° ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.\n",
        ")\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
        "print(\"INFO: ignore_mismatched_sizes=True ë¡œ ì„¤ì •í•˜ì—¬, ë¶„ë¥˜ê¸°(classifier) ë ˆì´ì–´ëŠ” ìƒˆë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "print(\"      ì´ì œ ì´ ë¶„ë¥˜ê¸° ë ˆì´ì–´ë¥¼ ìš°ë¦¬ì˜ ë°ì´í„°ì— ë§ê²Œ íŒŒì¸íŠœë‹í•˜ê²Œ ë©ë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DirtMJjkHbgS"
      },
      "outputs": [],
      "source": [
        "# Define the tokenization function\n",
        "def preprocess_data(examples):\n",
        "  # Tokenize the text. The 'text' column now includes metadata.\n",
        "  tokenized_batch = tokenizer(examples['text'], padding=True, truncation=True, max_length=512)\n",
        "  # Convert label data type to float32\n",
        "  tokenized_batch['labels'] = [[float(i) for i in label] for label in examples['labels']]\n",
        "  return tokenized_batch\n",
        "\n",
        "# Apply the tokenization function to the entire dataset\n",
        "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=['text'])\n",
        "\n",
        "print(\"âœ… Data tokenization complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l_x42u9fHwQS"
      },
      "outputs": [],
      "source": [
        "# 1. í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ ì •ì˜\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Sigmoidë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= 0.5)] = 1\n",
        "    y_true = labels\n",
        "\n",
        "    # F1 score (weighted average)\n",
        "    f1_weighted = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
        "    # Accuracy (ì „ì²´ ì˜ˆì¸¡ ì¤‘ ì •ë‹µ ë¹„ìœ¨)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    return {'f1': f1_weighted, 'accuracy': accuracy}\n",
        "\n",
        "# 2. í•™ìŠµ ì¸ì (TrainingArguments) ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    # [ìˆ˜ì •] ì¸ì ì´ë¦„ì„ 'eval_strategy'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# 3. Trainer ê°ì²´ ìƒì„±\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# 4. ëª¨ë¸ í•™ìŠµ ì‹œì‘\n",
        "print(\"\\nğŸ”¥ ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "trainer.train()\n",
        "\n",
        "# 5. ìµœì¢… ëª¨ë¸ ì €ì¥\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"\\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ! ìµœì¢… ëª¨ë¸ì´ '{output_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_e3XCzbKORB"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# ì´ì „ì— í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ëœ ê²½ë¡œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "# output_dir ë³€ìˆ˜ê°€ 6ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•œ ê²½ë¡œì™€ ë™ì¼í•œì§€ í™•ì¸í•˜ì„¸ìš”.\n",
        "print(f\"ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ: {output_dir}\")\n",
        "\n",
        "# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "try:\n",
        "    pipe = pipeline(\"text-classification\", model=output_dir, return_all_scores=True)\n",
        "    print(\"âœ… ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ!\")\n",
        "except OSError:\n",
        "    print(f\"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{output_dir}' ê²½ë¡œì— ëª¨ë¸ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì—¬ê¸°ì„œ ì‹¤í–‰ì„ ë©ˆì¶¥ë‹ˆë‹¤.\n",
        "    raise\n",
        "\n",
        "# --- í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥ ëª©ë¡ ---\n",
        "test_cases = [\n",
        "    {\n",
        "        \"comment_text\": \"ì™€, ì •ë§ ìœ ìµí•œ ì˜ìƒì´ë„¤ìš”! ê°ì‚¬í•©ë‹ˆë‹¤.\",\n",
        "        \"likes\": 15,\n",
        "        \"replies\": 2,\n",
        "        \"note\": \"ê¸ì •/ê°ì‚¬\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"ìê¾¸ ì´ìƒí•œ ì†Œë¦¬ë§Œ í•˜ë„¤. GPT ì§„ì§œ ë©ì²­í•´ì§€ëŠ”ë“¯.\",\n",
        "        \"likes\": 30,\n",
        "        \"replies\": 5,\n",
        "        \"note\": \"ë¶„ë…¸/ë¶ˆë§Œ\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"ì´ëŸ¬ë‹¤ ì§„ì§œ AIê°€ ì¸ê°„ì„ ì§€ë°°í•˜ëŠ” ì„¸ìƒì´ ì˜¤ëŠ” ê±° ì•„ë‹ê¹Œìš”? ë„ˆë¬´ ë¬´ì„œì›Œìš”.\",\n",
        "        \"likes\": 5,\n",
        "        \"replies\": 1,\n",
        "        \"note\": \"ê³µí¬/ë¶ˆì•ˆ\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"í˜ë“¤ ë•Œ AIí•œí…Œ ìœ„ë¡œë°›ëŠ”ë‹¤ëŠ” ëŒ“ê¸€ ë³´ë‹ˆê¹Œ ë§ˆìŒì´ ì¢€ ì§ í•˜ë„¤ìš”...\",\n",
        "        \"likes\": 45,\n",
        "        \"replies\": 3,\n",
        "        \"note\": \"ìŠ¬í””/ê³µê°\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"ì•ìœ¼ë¡œ AIê°€ ì–´ë–»ê²Œ ë°œì „í• ì§€ ê¶ê¸ˆí•˜ê³  ê¸°ëŒ€ë©ë‹ˆë‹¤.\",\n",
        "        \"likes\": 8,\n",
        "        \"replies\": 0,\n",
        "        \"note\": \"ê¸°ëŒ€/ì¤‘ë¦½\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"ì™€~ AI ë•ë¶„ì— ì´ì œ ëŒ“ê¸€ë„ ë¯¿ì„ ìˆ˜ê°€ ì—†ê² ë„¤. ì •ë§ ëŒ€ë‹¨í•˜ë‹¤.\",\n",
        "        \"likes\": 12,\n",
        "        \"replies\": 4,\n",
        "        \"note\": \"ëª¨í˜¸í•¨/ë°˜ì–´ë²•\"\n",
        "    },\n",
        "    {\n",
        "        \"comment_text\": \"ê°ê´€ì ì¸ ì •ë³´ ìš”ì•½ì€ ì˜í•˜ëŠ”ë°, ì¡°ê¸ˆë§Œ ë³µì¡í•´ì§€ë©´ ì—†ëŠ” ë§ì„ ì§€ì–´ë‚´ì„œ ë¬¸ì œì„.\",\n",
        "        \"likes\": 55,\n",
        "        \"replies\": 11,\n",
        "        \"note\": \"ë¹„íŒ (ë¶„ë…¸/ìŠ¬í””)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ ì¶”ë¡  ì‹¤í–‰ ë° ê²°ê³¼ ì¶œë ¥\n",
        "for i, case in enumerate(test_cases):\n",
        "    # í•™ìŠµ ë•Œ ì‚¬ìš©í•œ í˜•ì‹ê³¼ ë™ì¼í•˜ê²Œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±\n",
        "    input_text = f\"[ì¢‹ì•„ìš”: {case['likes']}, ë‹µê¸€: {case['replies']}] {case['comment_text']}\"\n",
        "\n",
        "    # ì¶”ë¡  ì‹¤í–‰\n",
        "    result = pipe(input_text)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"   í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{i+1} ({case['note']})\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"ì…ë ¥ ë¬¸ì¥: \\\"{input_text}\\\"\")\n",
        "    print(\"-\" * 20)\n",
        "    print(\"ê°ì • ë¶„ì„ ê²°ê³¼:\")\n",
        "\n",
        "    # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥\n",
        "    sorted_scores = sorted(result[0], key=lambda x: x['score'], reverse=True)\n",
        "    for r in sorted_scores:\n",
        "        print(f\"- {r['label']:<4s}: {r['score']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}